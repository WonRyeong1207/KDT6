{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Define Model Class\n",
    "- perents class: nn.Module\n",
    "- 필수 오버라이딩\n",
    "    - __init__(): 모델 층 구성, 전개\n",
    "    - forward(): 순방향 학습 진행 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optima\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchmetrics.regression import R2Score, MeanSquaredError\n",
    "from torchmetrics.classification import F1Score, Accuracy, ConfusionMatrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic create ANN model\n",
    "    - input layer: input node is len(feature)\n",
    "    - output layer: output node is len(label)\n",
    "    - hidden layer: fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model structure\n",
    "    - datasets: feature 4, label 1, regression\n",
    "    - input layer   : input 4,   output 20,  activation function: ReLU, sigmoid (To solve gradient vanishing problem)\n",
    "    - hidden layer  : input 10,  output 100, activation function: ReLu, sigmoid\n",
    "    - output layer  : input 100, output 1,   activation function: None (마지막에는 확률값을 기반으로 결과를 도출하기에, 분류는 필요함. sigmoid, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    # callback function\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(4, 20)\n",
    "        self.hidden_layer = nn.Linear(20, 100)\n",
    "        self.output_layer = nn.Linear(100, 1)\n",
    "        \n",
    "    # forward learning\n",
    "    # callback funtion\n",
    "    # 전달인자: 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel2(nn.Module):\n",
    "    # callback function\n",
    "    def __init__(self, in_feature, in_output, hid_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(in_feature, in_output)\n",
    "        self.hidden_layer = nn.Linear(in_output, hid_out)\n",
    "        self.output_layer = nn.Linear(hid_out, 1)\n",
    "        \n",
    "    # forward learning\n",
    "    # callback funtion\n",
    "    # 전달인자: 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer가 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    # callback function\n",
    "    def __init__(self, in_feature, in_output, hid_out_list):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(in_feature, in_output)\n",
    "        for i in range(len(hid_out_list)):\n",
    "            self.hidden_layer= nn.Linear(in_output, hid_out_list[i])\n",
    "        self.output_layer = nn.Linear(hid_out_list[-1], 1)\n",
    "        \n",
    "    # forward learning\n",
    "    # callback funtion\n",
    "    # 전달인자: 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model2 = MyModel2(10, 5, 2)\n",
    "model3 = MyModel3(5, 3, [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.0695, -0.4953,  0.4303,  0.2257],\n",
      "        [ 0.3295,  0.2683, -0.4400, -0.3547],\n",
      "        [-0.2076,  0.0292, -0.3534,  0.3305],\n",
      "        [-0.1421,  0.2729, -0.0192, -0.2853],\n",
      "        [-0.4648,  0.4872,  0.1413, -0.0553],\n",
      "        [-0.0408,  0.4473,  0.1172, -0.1417],\n",
      "        [ 0.4555, -0.1203,  0.0037,  0.2641],\n",
      "        [-0.1730, -0.4463,  0.2662, -0.1063],\n",
      "        [ 0.2347, -0.0956,  0.3067,  0.2818],\n",
      "        [ 0.1192, -0.4943, -0.1364,  0.3288],\n",
      "        [ 0.3479,  0.1239, -0.3115, -0.2110],\n",
      "        [ 0.3235,  0.1654,  0.0759,  0.0127],\n",
      "        [-0.2282,  0.2842,  0.4437,  0.2290],\n",
      "        [-0.0053, -0.3887, -0.3080,  0.3601],\n",
      "        [ 0.0940,  0.4270, -0.0519,  0.1230],\n",
      "        [-0.2904,  0.0997, -0.1626,  0.3619],\n",
      "        [-0.3796,  0.4605,  0.4616, -0.1958],\n",
      "        [-0.2408, -0.3312, -0.4171,  0.3473],\n",
      "        [-0.0736, -0.1310,  0.2940,  0.1350],\n",
      "        [ 0.2471,  0.2526,  0.1089,  0.0724]], requires_grad=True)\n",
      "[input_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.4836,  0.0976,  0.4918,  0.2660,  0.0035,  0.3017, -0.2856, -0.2947,\n",
      "         0.3753, -0.4874,  0.1940, -0.3754,  0.2036, -0.2525, -0.2803, -0.2303,\n",
      "        -0.2714,  0.2429,  0.0818,  0.4993], requires_grad=True)\n",
      "[hidden_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.1803, -0.2040, -0.0537,  ..., -0.2182, -0.2104, -0.0300],\n",
      "        [ 0.2064, -0.1639,  0.2209,  ..., -0.1681,  0.1495, -0.0261],\n",
      "        [-0.0585, -0.0624, -0.0826,  ..., -0.2045,  0.2191,  0.2233],\n",
      "        ...,\n",
      "        [-0.0363, -0.0800,  0.0300,  ...,  0.0947, -0.1504, -0.1369],\n",
      "        [-0.0507, -0.1268, -0.0204,  ..., -0.1555, -0.2209, -0.1281],\n",
      "        [ 0.1807, -0.0208,  0.2078,  ..., -0.0659,  0.1521,  0.1734]],\n",
      "       requires_grad=True)\n",
      "[hidden_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.0431,  0.2020, -0.2230, -0.2093,  0.1558,  0.0764, -0.1824,  0.2052,\n",
      "         0.0437, -0.1054,  0.2008,  0.0547,  0.1145, -0.0860,  0.1959, -0.2230,\n",
      "        -0.1928, -0.0288, -0.0309,  0.2052,  0.1868, -0.1122, -0.1707,  0.1453,\n",
      "         0.2005,  0.1530,  0.1802, -0.0362, -0.0707,  0.1967,  0.0197, -0.2012,\n",
      "         0.0347,  0.1937,  0.0575,  0.1031, -0.1576,  0.1359,  0.1893,  0.0854,\n",
      "        -0.2017,  0.0281, -0.2075,  0.2077, -0.1355, -0.1363, -0.1281,  0.1946,\n",
      "        -0.0191, -0.1569,  0.1158, -0.0585,  0.0870,  0.1254, -0.1629, -0.1294,\n",
      "        -0.0643, -0.0093,  0.2096,  0.0788,  0.2078, -0.1272,  0.1775, -0.1964,\n",
      "         0.1265, -0.1235,  0.1781, -0.0308,  0.0826,  0.1851, -0.0383, -0.1071,\n",
      "        -0.1722,  0.1655,  0.0456,  0.0393,  0.0138, -0.1963, -0.1897,  0.1699,\n",
      "         0.0247,  0.0773, -0.2107, -0.1893,  0.2038,  0.1823,  0.0885,  0.2077,\n",
      "         0.0260, -0.2204, -0.2070,  0.0400, -0.0607, -0.1279,  0.1878, -0.2029,\n",
      "         0.0165,  0.0988, -0.1655, -0.0007], requires_grad=True)\n",
      "[output_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.0794, -0.1000, -0.0934,  0.0814,  0.0007,  0.0466,  0.0555,  0.0507,\n",
      "          0.0565, -0.0683,  0.0952,  0.0080,  0.0458,  0.0558,  0.0769, -0.0469,\n",
      "          0.0747,  0.0957,  0.0314,  0.0100, -0.0249, -0.0500,  0.0678,  0.0173,\n",
      "          0.0082,  0.0095, -0.0101, -0.0575,  0.0739, -0.0745, -0.0406,  0.0405,\n",
      "         -0.0413, -0.0850,  0.0009,  0.0797,  0.0045, -0.0155,  0.0551,  0.0432,\n",
      "         -0.0405,  0.0969, -0.0493,  0.0766, -0.0500,  0.0498, -0.0414, -0.0272,\n",
      "         -0.0266,  0.0357, -0.0776,  0.0776, -0.0755,  0.0617,  0.0452,  0.0116,\n",
      "         -0.0426, -0.0339,  0.0443,  0.0835,  0.0031, -0.0936, -0.0150,  0.0503,\n",
      "          0.0199,  0.0289, -0.0313,  0.0396, -0.0381,  0.0870, -0.0363, -0.0161,\n",
      "          0.0556, -0.0842,  0.0907,  0.0206, -0.0569, -0.0404,  0.0536, -0.0742,\n",
      "          0.0351,  0.0502,  0.0120,  0.0924,  0.0365,  0.0502,  0.0910, -0.0048,\n",
      "         -0.0815, -0.0079,  0.0599, -0.0856,  0.0319, -0.0932,  0.0287, -0.0796,\n",
      "         -0.0010, -0.0118,  0.0352, -0.0436]], requires_grad=True)\n",
      "[output_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([0.0180], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"[{name}]\\n{param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel2(\n",
      "  (input_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (hidden_layer): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (output_layer): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[input_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.1659,  0.1956, -0.1988, -0.2665, -0.1695,  0.1738,  0.1246,  0.1946,\n",
      "         -0.2531, -0.1520],\n",
      "        [-0.0244, -0.1677, -0.1679,  0.1280,  0.2013, -0.0089, -0.2181, -0.1111,\n",
      "          0.1514, -0.1714],\n",
      "        [-0.0250, -0.2680, -0.1320, -0.2752,  0.2189,  0.0221, -0.0884,  0.1772,\n",
      "          0.1399,  0.0908],\n",
      "        [ 0.0640,  0.1548,  0.1983, -0.0498,  0.0526, -0.0113, -0.1887, -0.1653,\n",
      "         -0.0410,  0.0709],\n",
      "        [-0.2837,  0.2297,  0.2203,  0.0099,  0.2444, -0.1646,  0.1305, -0.1580,\n",
      "          0.0499, -0.1134]], requires_grad=True)\n",
      "\n",
      "[input_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.0760,  0.0111, -0.2650,  0.0834,  0.2917], requires_grad=True)\n",
      "\n",
      "[hidden_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.4262,  0.0449,  0.3284, -0.4234, -0.1793],\n",
      "        [-0.3358,  0.4111, -0.0788, -0.1198,  0.0795]], requires_grad=True)\n",
      "\n",
      "[hidden_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([0.2726, 0.3288], requires_grad=True)\n",
      "\n",
      "[output_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.6143,  0.6190]], requires_grad=True)\n",
      "\n",
      "[output_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.3225], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model2, end='\\n\\n')\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"[{name}]\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel3(\n",
      "  (input_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (hidden_layer): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (output_layer): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[input_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.2327, -0.4316, -0.4134,  0.1416, -0.1081],\n",
      "        [-0.3092, -0.3255, -0.1792,  0.1988, -0.4399],\n",
      "        [ 0.3149,  0.1753,  0.2194, -0.2917,  0.0767]], requires_grad=True)\n",
      "\n",
      "[input_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.2942, -0.3349, -0.0288], requires_grad=True)\n",
      "\n",
      "[hidden_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.4751,  0.2486, -0.2580],\n",
      "        [ 0.4404, -0.1092, -0.0283],\n",
      "        [-0.4737, -0.1809,  0.4953],\n",
      "        [ 0.3645, -0.1680, -0.1259],\n",
      "        [ 0.4183,  0.3358, -0.4177],\n",
      "        [ 0.3722,  0.0932,  0.2474]], requires_grad=True)\n",
      "\n",
      "[hidden_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.2075,  0.2741, -0.5120, -0.2056, -0.1812,  0.2413],\n",
      "       requires_grad=True)\n",
      "\n",
      "[output_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.2186, -0.3344, -0.0241, -0.4058, -0.4042,  0.1703]],\n",
      "       requires_grad=True)\n",
      "\n",
      "[output_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.1934], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model3, end='\\n\\n')\n",
    "for name, param in model3.named_parameters():\n",
    "    print(f\"[{name}]\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1565],\n",
      "        [-0.1835]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data_ts = torch.FloatTensor([[1, 3, 5, 7], [2, 4, 6, 8]])\n",
    "label_ts = torch.FloatTensor([[4], [5]])\n",
    "\n",
    "y_pred = model(data_ts)\n",
    "print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (input_layer): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (hidden_layer): Linear(in_features=20, out_features=100, bias=True)\n",
      "  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# summary(model, input_data=(4,))   # param 이 너무 많으면 안보여 주는 건가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
