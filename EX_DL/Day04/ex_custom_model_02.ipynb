{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Define Model Class\n",
    "- perents class: nn.Module\n",
    "- 필수 오버라이딩\n",
    "    - __init__(): 모델 층 구성, 전개\n",
    "    - forward(): 순방향 학습 진행 코드\n",
    " - 동적 모델\n",
    "    - container 모듈 둥 nn.ModuleList() 사용해서 동적으로 Layer 추가\n",
    "        - 연결이 안되어있다??\n",
    "        - Sequential은 연결되어있지만 저 친구는 아닌ㅁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optima\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchmetrics.regression import R2Score, MeanSquaredError\n",
    "from torchmetrics.classification import F1Score, Accuracy, ConfusionMatrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic create ANN model\n",
    "    - input layer: input node is len(feature)\n",
    "    - output layer: output node is len(label)\n",
    "    - hidden layer: fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model structure\n",
    "    - datasets: feature dynamic, label dynamic, regression\n",
    "    - input layer   : input dynamic,   output dynamic,  activation function: ReLU, sigmoid (To solve gradient vanishing problem)\n",
    "    - hidden layer  : dynamic, activation function: ReLu, sigmoid\n",
    "    - output layer  : input dynamic, output dynamic,   activation function: None (마지막에는 확률값을 기반으로 결과를 도출하기에, 분류는 필요함. sigmoid, softmax)\n",
    "    - input, output, hidden 모든것이 동적, hidden perceptron은 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "- 모델 이름: DynamicModel\n",
    "- 부모 class: nn.Module\n",
    "- parameter: in_in, out_out, h_inout, h_cnt\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer가 동적인 모델\n",
    "class DynamicMyModel(nn.Module):\n",
    "    # callback function\n",
    "    def __init__(self, in_in, out_out, h_inout, h_cnt):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(in_in, h_inout)\n",
    "        self.h_layers = nn.ModuleList([nn.Linear(h_inout, h_inout) for _ in range(h_cnt)])\n",
    "        # nn.ModuleList: 층을 동적으로 넣을때\n",
    "        self.output_layer = nn.Linear(h_inout, out_out)\n",
    "        \n",
    "    # forward learning\n",
    "    # callback funtion\n",
    "    # 전달인자: 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        y = self.input_layer(x)\n",
    "        y = F.relu(y)\n",
    "        for layer in self.h_layers:\n",
    "            y = layer(y)\n",
    "            y = F.relu(y)\n",
    "        y = self.output_layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicMyModel(3, 2, 5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.5249,  0.0662, -0.1112],\n",
      "        [ 0.1251, -0.4170,  0.0041],\n",
      "        [-0.5362, -0.5040, -0.3700],\n",
      "        [ 0.5637,  0.3859, -0.3439],\n",
      "        [ 0.5543, -0.1822,  0.2838]], requires_grad=True)\n",
      "\n",
      "[input_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.5349,  0.2100, -0.2777, -0.5025, -0.2382], requires_grad=True)\n",
      "\n",
      "[h_layers.0.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.0763, -0.0913,  0.4168, -0.0932,  0.1215],\n",
      "        [ 0.0350, -0.3118,  0.1556,  0.4144, -0.2095],\n",
      "        [ 0.1242,  0.3850, -0.0121, -0.4359,  0.3033],\n",
      "        [ 0.1386, -0.2237,  0.2826, -0.0336, -0.2064],\n",
      "        [ 0.3760, -0.1629, -0.2974,  0.2142, -0.1268]], requires_grad=True)\n",
      "\n",
      "[h_layers.0.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.0146, -0.1465,  0.0500, -0.1874, -0.2950], requires_grad=True)\n",
      "\n",
      "[h_layers.1.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.3180, -0.0118,  0.1843,  0.1905,  0.3439],\n",
      "        [ 0.0266, -0.2354, -0.1597, -0.1930, -0.3910],\n",
      "        [-0.4094,  0.0720,  0.0185,  0.0854,  0.3631],\n",
      "        [ 0.2598,  0.3954,  0.2704, -0.4385, -0.1009],\n",
      "        [ 0.0280, -0.3521,  0.4041,  0.2139,  0.1913]], requires_grad=True)\n",
      "\n",
      "[h_layers.1.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.0193, -0.1773,  0.0362,  0.1917, -0.4094], requires_grad=True)\n",
      "\n",
      "[h_layers.2.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.0059,  0.0380,  0.2934, -0.4070, -0.2018],\n",
      "        [-0.0497,  0.2038,  0.4242, -0.4213, -0.0243],\n",
      "        [ 0.0889,  0.0763,  0.4428, -0.0814, -0.0666],\n",
      "        [ 0.0016, -0.2039,  0.3608, -0.3807,  0.4312],\n",
      "        [ 0.3585, -0.1152, -0.3383, -0.3251,  0.4267]], requires_grad=True)\n",
      "\n",
      "[h_layers.2.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.3540, -0.2790, -0.2139,  0.1054, -0.4107], requires_grad=True)\n",
      "\n",
      "[h_layers.3.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.1067,  0.2259, -0.1366, -0.1100,  0.2633],\n",
      "        [ 0.0171, -0.1243,  0.3067,  0.3345,  0.4141],\n",
      "        [ 0.3197,  0.1102, -0.2549, -0.3492,  0.4048],\n",
      "        [ 0.3353, -0.2094, -0.3343, -0.4113,  0.0969],\n",
      "        [ 0.2121, -0.2204,  0.2256,  0.2156, -0.3904]], requires_grad=True)\n",
      "\n",
      "[h_layers.3.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.0423,  0.4020,  0.1930, -0.3397,  0.2685], requires_grad=True)\n",
      "\n",
      "[h_layers.4.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.0544,  0.3053, -0.0748, -0.1514,  0.1397],\n",
      "        [-0.3317, -0.2521,  0.0398,  0.3906,  0.0437],\n",
      "        [ 0.0377, -0.0017, -0.3490,  0.0879,  0.1039],\n",
      "        [ 0.0170,  0.2038,  0.1872,  0.1598, -0.3784],\n",
      "        [ 0.3023, -0.2744,  0.1122, -0.1531, -0.2382]], requires_grad=True)\n",
      "\n",
      "[h_layers.4.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.2077,  0.1987, -0.4364, -0.1473,  0.2631], requires_grad=True)\n",
      "\n",
      "[h_layers.5.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 3.9466e-01, -1.8562e-02,  1.2304e-01,  3.4134e-01, -4.0020e-01],\n",
      "        [-5.4809e-02, -1.2159e-05, -4.1847e-01, -2.7248e-01,  7.5723e-02],\n",
      "        [-2.5103e-01, -4.3413e-01, -2.0110e-01, -3.8178e-01,  1.9051e-01],\n",
      "        [ 2.9402e-01, -4.3921e-01,  1.0152e-01, -1.8940e-03, -2.8271e-01],\n",
      "        [ 3.4047e-02, -2.1151e-01, -9.1970e-02, -1.6126e-01, -7.2596e-02]],\n",
      "       requires_grad=True)\n",
      "\n",
      "[h_layers.5.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.1600,  0.0599, -0.3876, -0.1066,  0.0794], requires_grad=True)\n",
      "\n",
      "[h_layers.6.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.2232, -0.3896, -0.2777,  0.0373,  0.2651],\n",
      "        [ 0.4112, -0.2916, -0.0412,  0.2254,  0.3103],\n",
      "        [-0.3003,  0.1894, -0.3007, -0.2738, -0.1013],\n",
      "        [-0.2536, -0.0407,  0.0070, -0.3013, -0.1321],\n",
      "        [ 0.3616,  0.2869, -0.1005,  0.2756, -0.0577]], requires_grad=True)\n",
      "\n",
      "[h_layers.6.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.1616,  0.0994,  0.1638, -0.2912, -0.2890], requires_grad=True)\n",
      "\n",
      "[h_layers.7.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.2319, -0.3109, -0.4419, -0.2561,  0.3613],\n",
      "        [-0.0416,  0.4157, -0.0542, -0.4464, -0.3897],\n",
      "        [-0.2941, -0.1050, -0.1322, -0.0380,  0.3380],\n",
      "        [ 0.1596, -0.3999, -0.3866, -0.0133,  0.0711],\n",
      "        [-0.0785, -0.1319,  0.3042,  0.3469,  0.0862]], requires_grad=True)\n",
      "\n",
      "[h_layers.7.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.4041, -0.4459, -0.4187,  0.3117,  0.1528], requires_grad=True)\n",
      "\n",
      "[h_layers.8.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.3648,  0.4104,  0.0875, -0.2108,  0.4017],\n",
      "        [ 0.1093,  0.2290, -0.1719,  0.3919, -0.4461],\n",
      "        [-0.3856, -0.0575, -0.0619,  0.4103,  0.3736],\n",
      "        [-0.2243, -0.3413,  0.2906,  0.4010,  0.3060],\n",
      "        [ 0.3605, -0.0725, -0.1415,  0.3934,  0.0394]], requires_grad=True)\n",
      "\n",
      "[h_layers.8.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.4023,  0.0694,  0.3874,  0.1151,  0.2062], requires_grad=True)\n",
      "\n",
      "[h_layers.9.weight]\n",
      "Parameter containing:\n",
      "tensor([[-0.3152,  0.2717,  0.3786,  0.1708, -0.4034],\n",
      "        [ 0.0563, -0.4149,  0.4153, -0.2709, -0.2727],\n",
      "        [-0.2562,  0.3893, -0.0382, -0.3137,  0.2316],\n",
      "        [-0.1170,  0.1741,  0.2508, -0.3258, -0.2588],\n",
      "        [-0.1286, -0.0187,  0.4193,  0.1576,  0.4155]], requires_grad=True)\n",
      "\n",
      "[h_layers.9.bias]\n",
      "Parameter containing:\n",
      "tensor([-0.2543,  0.3550, -0.3929,  0.2529, -0.2470], requires_grad=True)\n",
      "\n",
      "[output_layer.weight]\n",
      "Parameter containing:\n",
      "tensor([[ 0.3561, -0.0616,  0.1652,  0.3701, -0.0766],\n",
      "        [-0.2143, -0.3444,  0.3311,  0.0913,  0.0786]], requires_grad=True)\n",
      "\n",
      "[output_layer.bias]\n",
      "Parameter containing:\n",
      "tensor([ 0.0276, -0.3926], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"[{name}]\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ts = torch.FloatTensor([[1, 3, 5], [2, 4, 6], [3, 5, 7], [4, 6, 8]])\n",
    "target_ts = torch.FloatTensor([[7, 9], [8, 10], [9, 11], [10, 18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0747, -0.4718],\n",
      "        [ 0.0747, -0.4718],\n",
      "        [ 0.0747, -0.4718],\n",
      "        [ 0.0747, -0.4718]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(data_ts)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
