{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터 유저와 mbti\n",
    "- datasets: mbti_label.csv, user_info.csv\n",
    "- machine learning method: supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_path = './data/user_info.csv'\n",
    "mbti_path = './data/mbti_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(user_path)\n",
    "mbti_df = pd.read_csv(mbti_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df['mbti_personality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('./data/feature.csv', encoding='utf-8')\n",
    "feature_df = feature_df.drop(columns=['Unnamed: 0'])\n",
    "label_sr = pd.read_csv('./data/target.csv', encoding='utf-8')\n",
    "label_sr = label_sr.drop(columns=['Unnamed: 0'])\n",
    "label_sr = label_sr['mbti_personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['number_of_tweets_scraped'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.astype({'number_of_tweets_scraped':'int64'})\n",
    "feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(500000, 500000))\n",
    "feature_df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계를 보기 위한\n",
    "data_df = feature_df.copy()\n",
    "data_df['mbti'] = label_sr\n",
    "\n",
    "names = data_df['mbti'].unique().tolist() \n",
    "mapping = {name:idx for idx, name in enumerate(names)}  # dict comprehension\n",
    "data_df['mbti_code'] = data_df['mbti'].map(mapping)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mbti = data_df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_mbti, annot=True, cmap='BuPu', fmt='.2f', linewidths=0.5)\n",
    "plt.title('mbti correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df2 = user_df.iloc[:, 7:]\n",
    "data_df2['mbti'] = label_sr\n",
    "\n",
    "names = data_df2['mbti'].unique().tolist() \n",
    "mapping = {name:idx for idx, name in enumerate(names)}  # dict comprehension\n",
    "data_df2['mbti_code'] = data_df2['mbti'].map(mapping)\n",
    "\n",
    "data_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mbti2 = data_df2.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr_mbti2, annot=True, cmap='BuPu', fmt='.2f', linewidths=0.5, cbar=False)\n",
    "plt.title('mbti correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상관관계가 높은 칼럼이 없음...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류모델을 뭘 쓸까 고민하다가... 한번쯤 해보고 싶었던 Support Vector Machine으로 Classification을 해보기로 결정\n",
    "- feature는 상관관계가 높은 것이 없어서 다른 조원들과 동일하게 total_~~ 를 사용.\n",
    "- label_sr은 data_df의 'mbti_code'를 사용\n",
    "- \n",
    "- \n",
    "- goal: mbti classification\n",
    "- learning method: supervised learning, classification\n",
    "- learning algorithm: support vector machine\n",
    "- feature: feature.csv\n",
    "- label: label encoded target.csv\n",
    "- scale: undetermined\n",
    "- scaler algorithm: standard, quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sr2 = data_df['mbti_code']\n",
    "\n",
    "# tarin : test = 8 : 2\n",
    "# random_state: 38\n",
    "# stratify: label_sr2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, label_sr2, test_size=0.2, stratify=label_sr2, random_state=38)\n",
    "\n",
    "print(f\"X train: {X_train.shape}, {X_train.ndim}D\")\n",
    "print(f\"y train: {y_train.shape}, {y_train.ndim}D\\n\")\n",
    "print(f\"X test: {X_test.shape}, {X_test.ndim}D\")\n",
    "print(f\"y test: {y_test.shape}, {y_test.ndim}D\\n\")\n",
    "print(f\"train test ratio: {len(X_train)/len(feature_df)*100:.2f} %, {len(X_test)/len(feature_df)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model insrance\n",
    "# default params use\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model attribute\n",
    "# svm_coef = svm_model.coef_    # model kernel=linear\n",
    "svm_dual_coef = svm_model.dual_coef_\n",
    "svm_bais = svm_model.intercept_\n",
    "svm_sv = svm_model.support_vectors_\n",
    "\n",
    "print(f\"dual coef:\\n{svm_dual_coef}\")\n",
    "print(f\"bais:\\n{svm_bais}\")\n",
    "print(f\"support vector:\\n{svm_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_score = svm_model.score(X_train, y_train)\n",
    "svm_test_score = svm_model.score(X_test, y_test)\n",
    "\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "svm_mat = confusion_matrix(y_test, svm_pred)\n",
    "svm_report = classification_report(y_test, svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"normal svm train score: {svm_train_score*100:.2f} %\")\n",
    "print(f\"normal svm test score: {svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "# print(f\"normal svm confusion matrix:\\n{svm_mat}\\n\")\n",
    "\n",
    "print(f\"normal svm accuracy: {svm_acc*100:.2f} %\")\n",
    "print(f\"normal svm classification report:\\n{svm_report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mbti 16개를 전부 분류를 하는 것은 무리라고 판단\n",
    "- 4개로 E/I, N/S, T/F, P/J로 각각 이진 분류를 하는 것을 목표로!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('./data/target_2.csv', encoding='utf-8')\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계를 보기 위한\n",
    "data_df = feature_df.copy()\n",
    "data_df['ei'] = label_df['ie']\n",
    "data_df['ns'] = label_df['ns']\n",
    "data_df['tf'] = label_df['ft']\n",
    "data_df['pj'] = label_df['jp']\n",
    "\n",
    "for mbti in ['ei', 'ns', 'tf', 'pj']:\n",
    "    names = data_df[mbti].unique().tolist() \n",
    "    mapping = {name:idx for idx, name in enumerate(names)}  # dict comprehension\n",
    "    data_df[mbti+'_code'] = data_df[mbti].map(mapping)\n",
    "\n",
    "data_df['ei'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mbti = data_df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_mbti, annot=True, cmap='BuPu', fmt='.2f', linewidths=0.5, cbar=False)\n",
    "plt.title('mbti correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여전히 상관관계는 금쪽이였다...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_ei = data_df['ei_code']\n",
    "# label_ns = data_df['ns_code']\n",
    "# label_tf = data_df['tf_code']\n",
    "# label_pj = data_df['pj_code']\n",
    "\n",
    "label_ei = data_df['ei']\n",
    "label_ns = data_df['ns']\n",
    "label_tf = data_df['tf']\n",
    "label_pj = data_df['pj']\n",
    "\n",
    "\n",
    "# tarin : test = 8 : 2\n",
    "# random_state: 38\n",
    "# stratify: label_...\n",
    "\n",
    "ei_X_train, ei_X_test, ei_y_train, ei_y_test = train_test_split(feature_df, label_ei, test_size=0.2, stratify=label_ei, random_state=38)\n",
    "ns_X_train, ns_X_test, ns_y_train, ns_y_test = train_test_split(feature_df, label_ns, test_size=0.2, stratify=label_ns, random_state=38)\n",
    "tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(feature_df, label_tf, test_size=0.2, stratify=label_tf, random_state=38)\n",
    "pj_X_train, pj_X_test, pj_y_train, pj_y_test = train_test_split(feature_df, label_pj, test_size=0.2, stratify=label_pj, random_state=38)\n",
    "\n",
    "print('ei 하나만')\n",
    "print(f\"EI X train: {ei_X_train.shape}, {ei_X_train.ndim}D\")\n",
    "print(f\"EI y train: {ei_y_train.shape}, {ei_y_train.ndim}D\\n\")\n",
    "print(f\"EI X test: {ei_X_test.shape}, {ei_X_test.ndim}D\")\n",
    "print(f\"EI y test: {ei_y_test.shape}, {ei_y_test.ndim}D\\n\")\n",
    "print(f\"EI train test ratio: {len(ei_X_train)/len(feature_df)*100:.2f} %, {len(ei_X_test)/len(feature_df)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model insrance\n",
    "# default params use\n",
    "ein_svm_model = SVC()\n",
    "ein_svm_model.fit(ei_X_train, ei_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model insrance\n",
    "# default params use\n",
    "nsn_svm_model = SVC()\n",
    "nsn_svm_model.fit(ns_X_train, ns_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model insrance\n",
    "# default params use\n",
    "tfn_svm_model = SVC()\n",
    "tfn_svm_model.fit(tf_X_train, tf_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model insrance\n",
    "# default params use\n",
    "pjn_svm_model = SVC()\n",
    "pjn_svm_model.fit(pj_X_train, pj_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model attribute\n",
    "# ein_svm_coef = ein_svm_model.coef_    # model kernel=linear\n",
    "ein_svm_dual_coef = ein_svm_model.dual_coef_\n",
    "ein_svm_bais = ein_svm_model.intercept_\n",
    "ein_svm_sv = ein_svm_model.support_vectors_\n",
    "\n",
    "nsn_svm_dual_coef = nsn_svm_model.dual_coef_\n",
    "nsn_svm_bais = nsn_svm_model.intercept_\n",
    "nsn_svm_sv = nsn_svm_model.support_vectors_\n",
    "\n",
    "tfn_svm_dual_coef = tfn_svm_model.dual_coef_\n",
    "tfn_svm_bais = tfn_svm_model.intercept_\n",
    "tfn_svm_sv = tfn_svm_model.support_vectors_\n",
    "\n",
    "pjn_svm_dual_coef = pjn_svm_model.dual_coef_\n",
    "pjn_svm_bais = pjn_svm_model.intercept_\n",
    "pjn_svm_sv = pjn_svm_model.support_vectors_\n",
    "\n",
    "print(f\"ei dual coef:\\n{ein_svm_dual_coef}\")\n",
    "print(f\"ei bais:\\n{ein_svm_bais}\")\n",
    "print(f\"ei support vector:\\n{ein_svm_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,save_file):\n",
    "    # vocab 저장\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ein_svm_train_score = ein_svm_model.score(ei_X_train, ei_y_train)\n",
    "ein_svm_test_score = ein_svm_model.score(ei_X_test, ei_y_test)\n",
    "\n",
    "ein_svm_pred = ein_svm_model.predict(ei_X_test)\n",
    "\n",
    "ein_svm_acc = accuracy_score(ei_y_test, ein_svm_pred)\n",
    "ein_svm_mat = confusion_matrix(ei_y_test, ein_svm_pred)\n",
    "ein_svm_report = classification_report(ei_y_test, ein_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsn_svm_train_score = nsn_svm_model.score(ns_X_train, ns_y_train)\n",
    "nsn_svm_test_score = nsn_svm_model.score(ns_X_test, ns_y_test)\n",
    "\n",
    "nsn_svm_pred = nsn_svm_model.predict(ns_X_test)\n",
    "\n",
    "nsn_svm_acc = accuracy_score(ns_y_test, nsn_svm_pred)\n",
    "nsn_svm_mat = confusion_matrix(ns_y_test, nsn_svm_pred)\n",
    "nsn_svm_report = classification_report(ns_y_test, nsn_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfn_svm_train_score = tfn_svm_model.score(tf_X_train, tf_y_train)\n",
    "tfn_svm_test_score = tfn_svm_model.score(tf_X_test, tf_y_test)\n",
    "\n",
    "tfn_svm_pred = tfn_svm_model.predict(tf_X_test)\n",
    "\n",
    "tfn_svm_acc = accuracy_score(tf_y_test, tfn_svm_pred)\n",
    "tfn_svm_mat = confusion_matrix(tf_y_test, tfn_svm_pred)\n",
    "tfn_svm_report = classification_report(tf_y_test, tfn_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjn_svm_train_score = pjn_svm_model.score(pj_X_train, pj_y_train)\n",
    "pjn_svm_test_score = pjn_svm_model.score(pj_X_test, pj_y_test)\n",
    "\n",
    "pjn_svm_pred = pjn_svm_model.predict(pj_X_test)\n",
    "\n",
    "pjn_svm_acc = accuracy_score(pj_y_test, pjn_svm_pred)\n",
    "pjn_svm_mat = confusion_matrix(pj_y_test, pjn_svm_pred)\n",
    "pjn_svm_report = classification_report(pj_y_test, pjn_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"normal E&I svm train score: {ein_svm_train_score*100:.2f} %\")\n",
    "print(f\"normal E&I svm test score: {ein_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(\"E:0 I:1\")\n",
    "print(f\"normal E&I svm confusion matrix:\\n{ein_svm_mat}\\n\")\n",
    "\n",
    "print(f\"normal E&I svm accuracy: {ein_svm_acc*100:.2f} %\")\n",
    "print(f\"normal E&I svm classification report:\\n{ein_svm_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"normal N&S svm train score: {nsn_svm_train_score*100:.2f} %\")\n",
    "print(f\"normal N&S svm test score: {nsn_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(\"N:0 S:1\")\n",
    "print(f\"normal N&S svm confusion matrix:\\n{nsn_svm_mat}\\n\")\n",
    "\n",
    "print(f\"normal N&S svm accuracy: {nsn_svm_acc*100:.2f} %\")\n",
    "print(f\"normal N&S svm classification report:\\n{nsn_svm_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"normal T&F svm train score: {tfn_svm_train_score*100:.2f} %\")\n",
    "print(f\"normal T&F svm test score: {tfn_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(\"F:0 T:1\")\n",
    "print(f\"normal T&F svm confusion matrix:\\n{tfn_svm_mat}\\n\")\n",
    "\n",
    "print(f\"normal T&F svm accuracy: {tfn_svm_acc*100:.2f} %\")\n",
    "print(f\"normal T&F svm classification report:\\n{tfn_svm_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"normal P&J svm train score: {pjn_svm_train_score*100:.2f} %\")\n",
    "print(f\"normal P&J svm test score: {pjn_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(\"J:0 p:1\")\n",
    "print(f\"normal P&J svm confusion matrix:\\n{pjn_svm_mat}\\n\")\n",
    "\n",
    "print(f\"normal P&J svm accuracy: {pjn_svm_acc*100:.2f} %\")\n",
    "print(f\"normal P&J svm classification report:\\n{pjn_svm_report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순서대로 55, 75, 55, 55 정도로 나옴.\n",
    "- params을 어떻게 건드릴까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scaling을 해달라고 해서 standatd만 일단 ㄱㄱ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_sd_scaler = StandardScaler()\n",
    "ei_sd_scaler.fit(ei_X_train, ei_y_train)\n",
    "\n",
    "ei_sd_X_train = ei_sd_scaler.transform(ei_X_train)\n",
    "ei_sd_X_test = ei_sd_scaler.transform(ei_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_sd_scaler = StandardScaler()\n",
    "ns_sd_scaler.fit(ns_X_train, ns_y_train)\n",
    "\n",
    "ns_sd_X_train = ns_sd_scaler.transform(ns_X_train)\n",
    "ns_sd_X_test = ns_sd_scaler.transform(ns_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sd_scaler = StandardScaler()\n",
    "tf_sd_scaler.fit(tf_X_train, tf_y_train)\n",
    "\n",
    "tf_sd_X_train = tf_sd_scaler.transform(tf_X_train)\n",
    "tf_sd_X_test = tf_sd_scaler.transform(tf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_sd_scaler = StandardScaler()\n",
    "pj_sd_scaler.fit(pj_X_train, pj_y_train)\n",
    "\n",
    "pj_sd_X_train = pj_sd_scaler.transform(pj_X_train)\n",
    "pj_sd_X_test = pj_sd_scaler.transform(pj_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'kernel':['poly', 'rbf', 'sigmoid']\n",
    "# 'max_iter':[-1, 0,  1]\n",
    "\n",
    "params = {'C':[0.01, 0.5, 0.1, 1.0, 2.5],'degree':[3, 4, 5, 6, 7, 8],\n",
    "          'decision_function_shape':['ovo', 'ovr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = 5\n",
    "ei_grid_model = GridSearchCV(SVC(), param_grid=params, cv=5, refit=True, return_train_score=True, verbose=2)\n",
    "ns_grid_model = GridSearchCV(SVC(), param_grid=params, cv=5, refit=True, return_train_score=True, verbose=2)\n",
    "tf_grid_model = GridSearchCV(SVC(), param_grid=params, cv=5, refit=True, return_train_score=True, verbose=2)\n",
    "pj_grid_model = GridSearchCV(SVC(), param_grid=params, cv=5, refit=True, return_train_score=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_grid_model.fit(ei_sd_X_train, ei_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_grid_model.fit(ns_sd_X_train, ns_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_grid_model.fit(tf_sd_X_train, tf_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_grid_model.fit(pj_sd_X_train, pj_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# ei_grid_model\n",
    "\n",
    "best_ei_svm_model = ei_grid_model.best_estimator_\n",
    "best_ei_param = ei_grid_model.best_params_\n",
    "ei_svm_result = ei_grid_model.cv_results_\n",
    "\n",
    "print(f\"Best E&I SVM model: {best_ei_svm_model}\")\n",
    "print(f\"Best E&I SVM Params: {best_ei_param}\")\n",
    "\n",
    "ei_svm_result_df = pd.DataFrame(ei_svm_result)\n",
    "ei_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# ns_grid_model\n",
    "\n",
    "best_ns_svm_model = ns_grid_model.best_estimator_\n",
    "best_ns_param = ns_grid_model.best_params_\n",
    "ns_svm_result = ns_grid_model.cv_results_\n",
    "\n",
    "print(f\"Best N&S SVM model: {best_ns_svm_model}\")\n",
    "print(f\"Best N&S SVM Params: {best_ns_param}\")\n",
    "\n",
    "ns_svm_result_df = pd.DataFrame(ns_svm_result)\n",
    "ns_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# tf_grid_model\n",
    "\n",
    "best_tf_svm_model = tf_grid_model.best_estimator_\n",
    "best_tf_param = tf_grid_model.best_params_\n",
    "tf_svm_result = tf_grid_model.cv_results_\n",
    "\n",
    "print(f\"Best T&F SVM model: {best_tf_svm_model}\")\n",
    "print(f\"Best T&F SVM Params: {best_tf_param}\")\n",
    "\n",
    "tf_svm_result_df = pd.DataFrame(tf_svm_result)\n",
    "tf_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# pj_grid_model\n",
    "\n",
    "best_pj_svm_model = pj_grid_model.best_estimator_\n",
    "best_pj_param = pj_grid_model.best_params_\n",
    "pj_svm_result = pj_grid_model.cv_results_\n",
    "\n",
    "print(f\"Best P&J SVM model: {best_pj_svm_model}\")\n",
    "print(f\"Best P&J SVM Params: {best_pj_param}\")\n",
    "\n",
    "pj_svm_result_df = pd.DataFrame(pj_svm_result)\n",
    "pj_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ei_svm_train_score = best_ei_svm_model.score(ei_sd_X_train, ei_y_train)\n",
    "best_ei_svm_test_score = best_ei_svm_model.score(ei_sd_X_test, ei_y_test)\n",
    "\n",
    "best_ei_svm_pred = best_ei_svm_model.predict(ei_sd_X_test)\n",
    "\n",
    "best_ei_svm_acc = accuracy_score(ei_y_test, best_ei_svm_pred)\n",
    "best_ei_svm_mat = confusion_matrix(ei_y_test, best_ei_svm_pred)\n",
    "best_ei_svm_report = classification_report(ei_y_test, best_ei_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ns_svm_train_score = best_ns_svm_model.score(ns_sd_X_train, ns_y_train)\n",
    "best_ns_svm_test_score = best_ns_svm_model.score(ns_sd_X_test, ns_y_test)\n",
    "\n",
    "best_ns_svm_pred = best_ns_svm_model.predict(ns_sd_X_test)\n",
    "\n",
    "best_ns_svm_acc = accuracy_score(ns_y_test, best_ns_svm_pred)\n",
    "best_ns_svm_mat = confusion_matrix(ns_y_test, best_ns_svm_pred)\n",
    "best_ns_svm_report = classification_report(ns_y_test, best_ns_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tf_svm_train_score = best_tf_svm_model.score(tf_sd_X_train, tf_y_train)\n",
    "best_tf_svm_test_score = best_tf_svm_model.score(tf_sd_X_test, tf_y_test)\n",
    "\n",
    "best_tf_svm_pred = best_tf_svm_model.predict(tf_sd_X_test)\n",
    "\n",
    "best_tf_svm_acc = accuracy_score(tf_y_test, best_tf_svm_pred)\n",
    "best_tf_svm_mat = confusion_matrix(tf_y_test, best_tf_svm_pred)\n",
    "best_tf_svm_report = classification_report(tf_y_test, best_tf_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pj_svm_train_score = best_pj_svm_model.score(pj_sd_X_train, pj_y_train)\n",
    "best_pj_svm_test_score = best_pj_svm_model.score(pj_sd_X_test, pj_y_test)\n",
    "\n",
    "best_pj_svm_pred = best_pj_svm_model.predict(pj_sd_X_test)\n",
    "\n",
    "best_pj_svm_acc = accuracy_score(pj_y_test, best_pj_svm_pred)\n",
    "best_pj_svm_mat = confusion_matrix(pj_y_test, best_pj_svm_pred)\n",
    "best_pj_svm_report = classification_report(pj_y_test, best_pj_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best E&I svm train score: {best_ei_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best E&I svm test score: {best_ei_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best E&I svm confusion matrix:\\n{best_ei_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best E&I svm accuracy: {best_ei_svm_acc*100:.2f} %\")\n",
    "print(f\"Best E&I svm classification report:\\n{best_ei_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best N&S svm train score: {best_ns_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best N&S svm test score: {best_ns_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best N&S svm confusion matrix:\\n{best_ns_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best N&S svm accuracy: {best_ns_svm_acc*100:.2f} %\")\n",
    "print(f\"Best N&S svm classification report:\\n{best_ns_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best T&F svm train score: {best_tf_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best T&F svm test score: {best_tf_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best T&F svm confusion matrix:\\n{best_tf_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best T&F svm accuracy: {best_tf_svm_acc*100:.2f} %\")\n",
    "print(f\"Best T&F svm classification report:\\n{best_tf_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best P&J svm train score: {best_pj_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best P&J svm test score: {best_pj_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best P&J svm confusion matrix:\\n{best_pj_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best P&J svm accuracy: {best_pj_svm_acc*100:.2f} %\")\n",
    "print(f\"Best P&J svm classification report:\\n{best_pj_svm_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오래 걸려서 안했던 kernel을 변경 시켜야 할듯... 튜닝의 의미가 없어졌음...\n",
    "- 반전으로 kernel은 오래 걸리는 것이 아니었음 !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'kernel':['poly', 'rbf', 'sigmoid']\n",
    "# 'max_iter':[-1, 0,  1]\n",
    "# 'degree':[3, 4, 5, 6, 7, 8] --> 3\n",
    "# 'decision_function_shape':['ovo', 'ovr'] --> ovo\n",
    "\n",
    "params = {'C':[0.01, 0.05, 0.1, 0.5, 1.0], 'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "          'decision_function_shape':['ovo', 'ovr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv: 10\n",
    "# randon_setate: 38\n",
    "\n",
    "ei_kn_model = GridSearchCV(SVC(random_state=38), param_grid=params, cv=10, refit=True, return_train_score=True, verbose=2)\n",
    "ns_kn_model = GridSearchCV(SVC(random_state=38), param_grid=params, cv=10, refit=True, return_train_score=True, verbose=2)\n",
    "tf_kn_model = GridSearchCV(SVC(random_state=38), param_grid=params, cv=10, refit=True, return_train_score=True, verbose=2)\n",
    "pj_kn_model = GridSearchCV(SVC(random_state=38), param_grid=params, cv=10, refit=True, return_train_score=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_kn_model.fit(ei_sd_X_train, ei_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_kn_model.fit(ns_sd_X_train, ns_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_kn_model.fit(tf_sd_X_train, tf_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_kn_model.fit(pj_sd_X_train, pj_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# ei_kn_model\n",
    "\n",
    "best_kn_ei_svm_model = ei_kn_model.best_estimator_\n",
    "best_kn_ei_param = ei_kn_model.best_params_\n",
    "ei_kn_svm_result = ei_kn_model.cv_results_\n",
    "\n",
    "print(f\"Best E&I SVM model: {best_kn_ei_svm_model}\")\n",
    "print(f\"Best E&I SVM Params: {best_kn_ei_param}\")\n",
    "\n",
    "ei_kn_svm_result_df = pd.DataFrame(ei_kn_svm_result)\n",
    "ei_kn_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# ns_kn_model\n",
    "\n",
    "best_kn_ns_svm_model = ns_kn_model.best_estimator_\n",
    "best_kn_ns_param = ns_kn_model.best_params_\n",
    "ns_kn_svm_result = ns_kn_model.cv_results_\n",
    "\n",
    "print(f\"Best N&S SVM model: {best_kn_ns_svm_model}\")\n",
    "print(f\"Best N&S SVM Params: {best_kn_ns_param}\")\n",
    "\n",
    "ns_kn_svm_result_df = pd.DataFrame(ns_kn_svm_result)\n",
    "ns_kn_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# tf_kn_model\n",
    "\n",
    "best_kn_tf_svm_model = tf_kn_model.best_estimator_\n",
    "best_kn_tf_param = tf_kn_model.best_params_\n",
    "tf_kn_svm_result = tf_kn_model.cv_results_\n",
    "\n",
    "print(f\"Best T&F SVM model: {best_kn_tf_svm_model}\")\n",
    "print(f\"Best T&F SVM Params: {best_kn_tf_param}\")\n",
    "\n",
    "tf_kn_svm_result_df = pd.DataFrame(tf_kn_svm_result)\n",
    "tf_kn_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch attiribute\n",
    "# pj_kn_model\n",
    "\n",
    "best_kn_pj_svm_model = pj_kn_model.best_estimator_\n",
    "best_kn_pj_param = pj_kn_model.best_params_\n",
    "pj_kn_svm_result = pj_kn_model.cv_results_\n",
    "\n",
    "print(f\"Best P&J SVM model: {best_kn_pj_svm_model}\")\n",
    "print(f\"Best P&J SVM Params: {best_kn_pj_param}\")\n",
    "\n",
    "pj_kn_svm_result_df = pd.DataFrame(pj_kn_svm_result)\n",
    "pj_kn_svm_result_df[['mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn_ei_svm_train_score = best_kn_ei_svm_model.score(ei_sd_X_train, ei_y_train)\n",
    "best_kn_ei_svm_test_score = best_kn_ei_svm_model.score(ei_sd_X_test, ei_y_test)\n",
    "\n",
    "best_kn_ei_svm_pred = best_ei_svm_model.predict(ei_sd_X_test)\n",
    "\n",
    "best_kn_ei_svm_acc = accuracy_score(ei_y_test, best_kn_ei_svm_pred)\n",
    "best_kn_ei_svm_mat = confusion_matrix(ei_y_test, best_kn_ei_svm_pred)\n",
    "best_kn_ei_svm_report = classification_report(ei_y_test, best_kn_ei_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn_ns_svm_train_score = best_kn_ns_svm_model.score(ns_sd_X_train, ns_y_train)\n",
    "best_kn_ns_svm_test_score = best_kn_ns_svm_model.score(ns_sd_X_test, ns_y_test)\n",
    "\n",
    "best_kn_ns_svm_pred = best_kn_ns_svm_model.predict(ns_sd_X_test)\n",
    "\n",
    "best_kn_ns_svm_acc = accuracy_score(ns_y_test, best_kn_ns_svm_pred)\n",
    "best_kn_ns_svm_mat = confusion_matrix(ns_y_test, best_kn_ns_svm_pred)\n",
    "best_kn_ns_svm_report = classification_report(ns_y_test, best_ns_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn_tf_svm_train_score = best_kn_tf_svm_model.score(tf_sd_X_train, tf_y_train)\n",
    "best_kn_tf_svm_test_score = best_kn_tf_svm_model.score(tf_sd_X_test, tf_y_test)\n",
    "\n",
    "best_kn_tf_svm_pred = best_tf_svm_model.predict(tf_sd_X_test)\n",
    "\n",
    "best_kn_tf_svm_acc = accuracy_score(tf_y_test, best_kn_tf_svm_pred)\n",
    "best_kn_tf_svm_mat = confusion_matrix(tf_y_test, best_kn_tf_svm_pred)\n",
    "best_kn_tf_svm_report = classification_report(tf_y_test, best_kn_tf_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn_pj_svm_train_score = best_kn_pj_svm_model.score(pj_sd_X_train, pj_y_train)\n",
    "best_kn_pj_svm_test_score = best_kn_pj_svm_model.score(pj_sd_X_test, pj_y_test)\n",
    "\n",
    "best_kn_pj_svm_pred = best_pj_svm_model.predict(pj_sd_X_test)\n",
    "\n",
    "best_kn_pj_svm_acc = accuracy_score(pj_y_test, best_kn_pj_svm_pred)\n",
    "best_kn_pj_svm_mat = confusion_matrix(pj_y_test, best_kn_pj_svm_pred)\n",
    "best_kn_pj_svm_report = classification_report(pj_y_test, best_kn_pj_svm_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best kernel E&I svm train score: {best_kn_ei_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best kernel E&I svm test score: {best_kn_ei_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best kernel E&I svm confusion matrix:\\n{best_kn_ei_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best kernel E&I svm accuracy: {best_kn_ei_svm_acc*100:.2f} %\")\n",
    "print(f\"Best kernel E&I svm classification report:\\n{best_kn_ei_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best kernel N&S svm train score: {best_kn_ns_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best kernel N&S svm test score: {best_kn_ns_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best kernel N&S svm confusion matrix:\\n{best_kn_ns_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best kernel N&S svm accuracy: {best_kn_ns_svm_acc*100:.2f} %\")\n",
    "print(f\"Best kernel N&S svm classification report:\\n{best_kn_ns_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best kernel T&F svm train score: {best_kn_tf_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best kernel T&F svm test score: {best_kn_tf_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best kernel T&F svm confusion matrix:\\n{best_kn_tf_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best kernel T&F svm accuracy: {best_kn_tf_svm_acc*100:.2f} %\")\n",
    "print(f\"Best kernel T&F svm classification report:\\n{best_kn_tf_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best kernel P&J svm train score: {best_kn_pj_svm_train_score*100:.2f} %\")\n",
    "print(f\"Best kernel P&J svm test score: {best_kn_pj_svm_test_score*100:.2f} %\\n\")\n",
    "\n",
    "print(f\"Best kernel P&J svm confusion matrix:\\n{best_kn_pj_svm_mat}\\n\")\n",
    "\n",
    "print(f\"Best kernel P&J svm accuracy: {best_kn_pj_svm_acc*100:.2f} %\")\n",
    "print(f\"Best kernel P&J svm classification report:\\n{best_kn_pj_svm_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(ei_sd_scaler,'./scaler/ei_sd_scaler')\n",
    "save_model(ns_sd_scaler,'./scaler/ns_sd_scaler')\n",
    "save_model(tf_sd_scaler,'./scaler/tf_sd_scaler')\n",
    "save_model(pj_sd_scaler,'./scaler/pj_sd_scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_kn_ei_svm_model,'./model/ei_model')\n",
    "save_model(best_kn_ns_svm_model,'./model/ns_model')\n",
    "save_model(best_kn_tf_svm_model,'./model/tf_model')\n",
    "save_model(best_kn_pj_svm_model,'./model/pj_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- thresholld를 바꿔보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_by_threshold(model, y_test, pred_prob, thresholds):\n",
    "    for custom_thresholod in thresholds:\n",
    "        binarizer = Binarizer(threshold=custom_thresholod).fit(pred_prob)\n",
    "        custom_pred = binarizer.transform(pred_prob)\n",
    "        \n",
    "        acc = accuracy_score(y_test, custom_pred)\n",
    "        mat = confusion_matrix(y_test,custom_pred)\n",
    "        report = classification_report(y_test, custom_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"threshold: {custom_thresholod}\")\n",
    "        print(f\"{model} confusion matrix:\\n{mat}\\n\")\n",
    "        print(f\"{model} accuracy: {acc*100:.2f} %\")\n",
    "        print(f\"{model} classification report:\\n{report}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_ei = data_df['ei_code']\n",
    "# label_ns = data_df['ns_code']\n",
    "# label_tf = data_df['tf_code']\n",
    "# label_pj = data_df['pj_code']\n",
    "\n",
    "label_ei = data_df['ei']\n",
    "label_ns = data_df['ns']\n",
    "label_tf = data_df['tf']\n",
    "label_pj = data_df['pj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_X_train, ei_X_test, ei_y_train, ei_y_test = train_test_split(feature_df, label_ei, test_size=0.2, stratify=label_ei, random_state=38)\n",
    "ns_X_train, ns_X_test, ns_y_train, ns_y_test = train_test_split(feature_df, label_ns, test_size=0.2, stratify=label_ns, random_state=38)\n",
    "tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(feature_df, label_tf, test_size=0.2, stratify=label_tf, random_state=38)\n",
    "pj_X_train, pj_X_test, pj_y_train, pj_y_test = train_test_split(feature_df, label_pj, test_size=0.2, stratify=label_pj, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_sd_scaler = StandardScaler()\n",
    "ei_sd_scaler.fit(ei_X_train, ei_y_train)\n",
    "\n",
    "ei_sd_X_train = ei_sd_scaler.transform(ei_X_train)\n",
    "ei_sd_X_test = ei_sd_scaler.transform(ei_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_sd_scaler = StandardScaler()\n",
    "ns_sd_scaler.fit(ns_X_train, ns_y_train)\n",
    "\n",
    "ns_sd_X_train = ns_sd_scaler.transform(ns_X_train)\n",
    "ns_sd_X_test = ns_sd_scaler.transform(ns_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sd_scaler = StandardScaler()\n",
    "tf_sd_scaler.fit(tf_X_train, tf_y_train)\n",
    "\n",
    "tf_sd_X_train = tf_sd_scaler.transform(tf_X_train)\n",
    "tf_sd_X_test = tf_sd_scaler.transform(tf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_sd_scaler = StandardScaler()\n",
    "pj_sd_scaler.fit(pj_X_train, pj_y_train)\n",
    "\n",
    "pj_sd_X_train = pj_sd_scaler.transform(pj_X_train)\n",
    "pj_sd_X_test = pj_sd_scaler.transform(pj_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_model = SVC(C=1.0, decision_function_shape='ovo', probability=True, kernel='rbf')\n",
    "ei_model.fit(ei_sd_X_train, ei_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_train_score = ei_model.score(ei_sd_X_train, y_train)\n",
    "ei_test_score = ei_model.score(ei_sd_X_test, y_test)\n",
    "\n",
    "print(f\"ei model tarin score: {ei_train_score*100:.2f} %\")\n",
    "print(f\"ei model test score: {ei_test_score*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_proba = ei_model.predict_proba(ei_sd_X_test)\n",
    "thresholds = [x/10 for x in range(1, 11)]\n",
    "get_eval_by_threshold(ei_model, ei_y_test, ei_proba[:,1].reshape(-1,1), thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [x for x in range(1, 11)]\n",
    "get_eval_by_threshold(ei_model, ei_y_test, ei_proba[:,1].reshape(-1,1), thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_model = SVC(C=0.05, decision_function_shape='ovo', probability=True, kernel='poly')\n",
    "ns_model.fit(ns_sd_X_train, ns_y_train)\n",
    "\n",
    "ns_train_score = ns_model.score(ns_sd_X_train, y_train)\n",
    "ns_test_score = ns_model.score(ns_sd_X_test, y_test)\n",
    "\n",
    "print(f\"ns model tarin score: {ns_train_score*100:.2f} %\")\n",
    "print(f\"ns model test score: {ns_test_score*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_proba = ns_model.predict_proba(ns_sd_X_test)\n",
    "thresholds = [x/10 for x in range(1, 11)]\n",
    "thresholds2 = [x for x in range(1, 11)]\n",
    "\n",
    "get_eval_by_threshold(ns_model, ns_y_test, ns_proba[:,1].reshape(-1,1), thresholds)\n",
    "get_eval_by_threshold(ns_model, ns_y_test, ns_proba[:,1].reshape(-1,1), thresholds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
