{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 책 예제\n",
    "- 네이버 영화 리뷰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optima\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout=0.5, bidirectional=True, model_type='lstm'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                proj_size=0\n",
    "            )\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\PC\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\PC\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                  |   label |\n",
      "|------:|:----------------------------------------------------------------------|--------:|\n",
      "| 16477 | 인간미 넘치는 법정드라마..                                            |       1 |\n",
      "|  5969 | 장선우 감독과 이정현의 처음이자 마지막의 역작                         |       1 |\n",
      "| 46459 | 류승완,류승범의 풋풋하던 시절. 영화는 극사실적으로 표현하려고 애쓴듯. |       0 |\n",
      "| 33297 | 송승헌, 김희선이 출연한 영화는 그냥 안타깝다. 영화와 인연이 아닌 듯   |       0 |\n",
      "| 35047 | 공포보단 코미다가 더 가깝다는..                                       |       1 |\n",
      "train size: 45000\n",
      "test size: 5000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=4)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(f\"train size: {len(train)}\")\n",
    "print(f\"test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tokenization & create vocab\n",
    "def build_vocab(corpus, n_vocab, spacial_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = spacial_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '에', '가', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, spacial_tokens=['<pad>', '<unk>'])\n",
    "token_to_id = {token:idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx:token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intager encoding & padding\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1 1356 4829   51    6    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[  1  13   1  61 147  13 147   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "unk_id = token_to_id['<unk>']\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dataloader\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function & optimizer\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layer = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier = SentenceClassifier(n_vocab, hidden_dim, embedding_dim, n_layer).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optima.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    \n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % interval == 0:\n",
    "            print(f\"train loss {step}: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model test\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "    \n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits) > 0.5\n",
    "        corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "    print(f\"validation loss: {np.mean(losses)}, accuracy: {np.mean(corrects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0: 0.6933217644691467\n",
      "train loss 500: 0.6943136583307308\n",
      "train loss 1000: 0.6905747528200026\n",
      "train loss 1500: 0.6749273884264649\n",
      "train loss 2000: 0.6555213789562175\n",
      "train loss 2500: 0.6367976230556895\n",
      "validation loss: 0.5279639783187415, accuracy: 0.7494\n",
      "train loss 0: 0.31776538491249084\n",
      "train loss 500: 0.5169963894668453\n",
      "train loss 1000: 0.5211648493707478\n",
      "train loss 1500: 0.5198383837501499\n",
      "train loss 2000: 0.5221993933091575\n",
      "train loss 2500: 0.5202475592690627\n",
      "validation loss: 0.5058195431487629, accuracy: 0.7298\n",
      "train loss 0: 0.5961745977401733\n",
      "train loss 500: 0.48100612843464946\n",
      "train loss 1000: 0.4821624792033023\n",
      "train loss 1500: 0.48279716428878067\n",
      "train loss 2000: 0.4821023522750072\n",
      "train loss 2500: 0.4799151421367812\n",
      "validation loss: 0.4666051073862722, accuracy: 0.7894\n",
      "train loss 0: 0.18460184335708618\n",
      "train loss 500: 0.43342965932008987\n",
      "train loss 1000: 0.4310774285342548\n",
      "train loss 1500: 0.4307644139729604\n",
      "train loss 2000: 0.4270257778290747\n",
      "train loss 2500: 0.4231610035315984\n",
      "validation loss: 0.4471454216649357, accuracy: 0.8078\n",
      "train loss 0: 0.31813526153564453\n",
      "train loss 500: 0.37653847017925896\n",
      "train loss 1000: 0.379422373057424\n",
      "train loss 1500: 0.3829799690181458\n",
      "train loss 2000: 0.38177963870873993\n",
      "train loss 2500: 0.38239367692614024\n",
      "validation loss: 0.41739500471102164, accuracy: 0.814\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리메이크 [ 0.48094285 -0.83728343 -0.31191462  0.2117208   0.04564255 -0.596668\n",
      "  1.0199872   0.20349438  0.30593103  0.5277261   0.1662459   0.89788216\n",
      "  0.1946853   1.195648    0.8976036  -1.0416138  -0.58721733  0.6944218\n",
      " -0.46866128 -0.60765386 -2.1934917  -0.4531325   1.1143396   0.37227768\n",
      " -1.4145187   0.2735119  -0.14097662  1.9225914   0.45480484 -0.17459026\n",
      "  1.5516007   0.2936152   0.64963645 -0.4215774   0.74884295 -0.6422965\n",
      " -0.6698972   0.30147874  0.16571277  0.19205159 -0.5371739  -1.991847\n",
      " -0.27439493 -0.7088234   1.4496316  -0.41890925 -0.76643884  0.88281\n",
      " -0.8680259   1.3656698   0.11580013 -0.35289472  1.0115812   0.29589006\n",
      " -0.82661825 -0.01542456 -0.58443356 -0.18068504  0.8624229   0.44413677\n",
      "  0.1760467   1.1405487   0.9097203  -0.6424394   0.10887486  1.4188663\n",
      "  0.769108   -1.9375563   1.9752502   0.12015982 -0.8225379  -2.1790164\n",
      "  0.02257181 -1.6617163   0.9023272  -0.54046655 -2.1477268   1.0831587\n",
      " -0.38413635 -0.44572026 -0.25716084  0.24758536 -0.53695166  0.16550133\n",
      " -0.5511943   1.1241293   0.24907129  0.30807427 -0.68915147  0.67692083\n",
      " -1.4789082   0.8220426   0.87612754 -1.0984247  -1.5438989  -0.07221632\n",
      "  0.8938364   0.72327495 -0.56119835 -1.9301127  -0.782914    0.04463778\n",
      " -0.23381943  1.3099841  -0.5897292  -0.9333827  -0.04644863  1.3056635\n",
      "  0.03677269  0.56119573  0.78291684 -0.622199    1.3089575  -0.46112925\n",
      " -0.11022086 -0.9228954  -1.3766514  -0.68428415  0.99580926 -0.7734304\n",
      " -0.17614536 -0.10347046  0.3998581   0.19784209 -0.41645685 -0.8520869\n",
      " -0.7293173  -1.3544922 ]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_38_018",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
